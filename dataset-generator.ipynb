{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9906114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "from matplotlib import patches\n",
    "import zipfile\n",
    "from pycocotools.coco import COCO\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random as rn\n",
    "from shutil import copyfile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\n",
    "!mkdir data\n",
    "!mv *.zip data/\n",
    "\n",
    "# # Unzip data\n",
    "with zipfile.ZipFile('data/train2017.zip', 'r') as zip_ref: zip_ref.extractall('data/')\n",
    "with zipfile.ZipFile('data/val2017.zip', 'r') as zip_ref: zip_ref.extractall('data/')\n",
    "with zipfile.ZipFile('data/annotations_trainval2017.zip', 'r') as zip_ref: zip_ref.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074edc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.91s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load annotations for training data\n",
    "train_annot_path = 'data/annotations/person_keypoints_train2017.json'\n",
    "val_annot_path = 'data/annotations/person_keypoints_val2017.json'\n",
    "train_coco = COCO(train_annot_path) # load annotations for training set\n",
    "val_coco = COCO(val_annot_path) # load annotations for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5251ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load meta data on images: https://towardsdatascience.com/how-to-analyze-the-coco-dataset-for-pose-estimation-7296e2ffb12e\n",
    "def get_meta(coco):\n",
    "    ids = list(coco.imgs.keys())\n",
    "    for i, img_id in enumerate(ids):\n",
    "        img_meta = coco.imgs[img_id]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # basic parameters of an image\n",
    "        img_file_name = img_meta['file_name']\n",
    "        w = img_meta['width']\n",
    "        h = img_meta['height']\n",
    "        # retrieve metadata for all persons in the current image\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        yield [img_id, img_file_name, w, h, anns]\n",
    "        \n",
    "# Convert Coco object to a pandas dataframe\n",
    "def convert_to_df(coco):\n",
    "    images_data = []\n",
    "    persons_data = []\n",
    "    # iterate over all images\n",
    "    for img_id, img_fname, w, h, meta in get_meta(coco):\n",
    "        images_data.append({\n",
    "            'image_id': int(img_id),\n",
    "            'path': img_fname,\n",
    "            'width': int(w),\n",
    "            'height': int(h)\n",
    "        })\n",
    "        # iterate over all metadata\n",
    "        for m in meta:\n",
    "            persons_data.append({\n",
    "                'image_id': m['image_id'],\n",
    "                'is_crowd': m['iscrowd'],\n",
    "                'bbox': m['bbox'],\n",
    "                'area': m['area'],\n",
    "                'num_keypoints': m['num_keypoints'],\n",
    "                'keypoints': m['keypoints'],\n",
    "            })\n",
    "    # create dataframe with image paths\n",
    "    images_df = pd.DataFrame(images_data)\n",
    "    images_df.set_index('image_id', inplace=True)\n",
    "    # create dataframe with persons\n",
    "    persons_df = pd.DataFrame(persons_data)\n",
    "    persons_df.set_index('image_id', inplace=True)\n",
    "    return images_df, persons_df\n",
    "\n",
    "# Create train and validation dfs\n",
    "images_df, persons_df = convert_to_df(train_coco)\n",
    "train_coco_df = pd.merge(images_df, persons_df, right_index=True, left_index=True)\n",
    "\n",
    "images_df, persons_df = convert_to_df(val_coco)\n",
    "val_coco_df = pd.merge(images_df, persons_df, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6fc30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149813\n",
      "6352\n"
     ]
    }
   ],
   "source": [
    "# Drop all photos with no keypoints or where a group is just marked as crowd\n",
    "# This removes unwanted training data\n",
    "train_coco_df = train_coco_df[train_coco_df['is_crowd']==0]\n",
    "train_coco_df = train_coco_df[train_coco_df['num_keypoints']>0]\n",
    "\n",
    "val_coco_df = val_coco_df[val_coco_df['is_crowd']==0]\n",
    "val_coco_df = val_coco_df[val_coco_df['num_keypoints']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de7e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing transform defined in the DeepPose paper\n",
    "# This moves pixel coordinates into a relative scale of -0.5 to 0.5\n",
    "def N(y, b):\n",
    "    return [ (y[0]-b[0]) * 1/b[2], (y[1]-b[1]) * 1/b[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ba6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moves keypoints to the correct location after: cropping, scaling, and normalizing photos\n",
    "def convertKeyPoints(df, dimensions, source, index):\n",
    "    bbox = np.array(df.iloc[index][\"bbox\"]).astype(np.int64)\n",
    "    keypoints = df.iloc[index][\"keypoints\"]\n",
    "\n",
    "    bbox_tl_x = bbox[0]\n",
    "    bbox_tl_y = bbox[1]\n",
    "    w_original = bbox[2]\n",
    "    h_original = bbox[3]\n",
    "\n",
    "    bbox_rescale = [dimensions/2, dimensions/2, dimensions, dimensions]\n",
    "    \n",
    "    keypoints = np.array(keypoints).reshape(-1,3)\n",
    "    x = keypoints[:,0]\n",
    "    y = keypoints[:,1]\n",
    "    v = keypoints[:,2]\n",
    "    \n",
    "    # Scale and move keypoints\n",
    "    x = (x - bbox_tl_x) * dimensions / w_original\n",
    "    y = (y - bbox_tl_y)* dimensions / h_original\n",
    "    \n",
    "    # Do normalization\n",
    "    x = (x-bbox_rescale[0]) * 1 / bbox_rescale[2]\n",
    "    y = (y-bbox_rescale[1]) * 1 / bbox_rescale[3]\n",
    "    \n",
    "    # Set visible keypoint v to 1 and nonvisible or not in image to 0\n",
    "    no_point_indices = np.where(v < 2)[0]\n",
    "    v = np.ones_like(v)\n",
    "    v[no_point_indices] = 0\n",
    "    x[no_point_indices] = 0\n",
    "    y[no_point_indices] = 0\n",
    "    \n",
    "    # if keypoints are outside of the bounding box set their x,y,v to 0\n",
    "    outside_bbox_x_indices = np.where((x<-0.5) | (x>0.5))\n",
    "    outside_bbox_y_indices = np.where((y<-0.5) | (y>0.5))\n",
    "    v[outside_bbox_x_indices] = 0\n",
    "    v[outside_bbox_y_indices] = 0\n",
    "    x[outside_bbox_x_indices] = 0\n",
    "    x[outside_bbox_y_indices] = 0\n",
    "    x[outside_bbox_x_indices] = 0\n",
    "    y[outside_bbox_y_indices] = 0\n",
    "    \n",
    "    \n",
    "    keypoints = np.column_stack((x,y,v)).reshape(-1)\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84fab417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales and crops images\n",
    "def convertImg(df, dimensions, source, index):\n",
    "    img = image.imread(f'data/{source}2017/{df.iloc[index][\"path\"].replace(\"c\", \"\")}')\n",
    "    bbox = np.array(df.iloc[index][\"bbox\"]).astype(np.int64)\n",
    "    \n",
    "    # Account for potential gray images by adding channels\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        if (len(img.shape) == 2): img = np.expand_dims(img,-1)\n",
    "        img = cv2.merge([img,img,img])\n",
    "\n",
    "    # Crop image to bbox\n",
    "    img = img[bbox[1]:bbox[1]+bbox[3],bbox[0]:bbox[0]+bbox[2]]\n",
    "\n",
    "    # Scale image \n",
    "    img = cv2.resize(img, dsize=(dimensions, dimensions), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    img = img.astype('uint8') # enforce int \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5dba6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates final dataframes\n",
    "train_df = pd.DataFrame(columns = ['path', 'width', 'height', 'keypoints', 'num_keypoints', 'bbox'])\n",
    "val_df = pd.DataFrame(columns = ['path', 'width', 'height', 'keypoints', 'num_keypoints', 'bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fc8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p train2017\n",
    "!mkdir -p val2017\n",
    "\n",
    "# Compute new keypoints for train_df\n",
    "for row in range(len(train_coco_df)):\n",
    "    df_row = train_coco_df.iloc[row]\n",
    "    kps = convertKeyPoints(train_coco_df, 224, 'train', row)\n",
    "    \n",
    "    # Rename from the same path if paths matching\n",
    "    same_path = train_df.loc[train_df['path'].str.contains(df_row['path'])]\n",
    "    \n",
    "    path = df_row['path']\n",
    "    if len(same_path) > 0:\n",
    "        path = 'c' + same_path.iloc[-1]['path']\n",
    "        \n",
    "    train_df = train_df.append(\n",
    "        {\n",
    "            'path' : path,\n",
    "            'width' : 224,\n",
    "            'height' : 224,\n",
    "            'keypoints' : kps,\n",
    "            'num_keypoints' : sum(1 for _ in filter(lambda score: score > -1, kps)),\n",
    "            'bbox' : df_row['bbox']\n",
    "        }\n",
    "        , ignore_index = True)\n",
    "    if row % 1000 == 0: print(f'row {row}')\n",
    "    \n",
    "# Compute new keypoints for val_df\n",
    "for row in range(len(val_coco_df)):\n",
    "    df_row = val_coco_df.iloc[row]\n",
    "    kps = convertKeyPoints(val_coco_df, 224, 'val', row)\n",
    "    \n",
    "    # Rename from the same path if paths matching\n",
    "    same_path = val_df.loc[val_df['path'].str.contains(df_row['path'])]\n",
    "    \n",
    "    path = df_row['path']\n",
    "    if len(same_path) > 0:\n",
    "        path = 'c' + same_path.iloc[-1]['path']\n",
    "    \n",
    "    val_df = val_df.append(\n",
    "        {\n",
    "            'path' : path,\n",
    "            'width' : 224,\n",
    "            'height' : 224,\n",
    "            'keypoints' : kps,\n",
    "            'num_keypoints' : sum(1 for _ in filter(lambda score: score > -1, kps)),\n",
    "            'bbox' : df_row['bbox']\n",
    "        }\n",
    "        , ignore_index = True)\n",
    "    if row % 1000 == 0: print(f'row {row}')\n",
    "    \n",
    "# Create new cropped files in data-2\n",
    "for row in range(len(train_df)):\n",
    "    df_row = train_df.iloc[row]\n",
    "    img = convertImg(train_df, 224, 'train', row)\n",
    "    cv2.imwrite(f'train2017/{train_df.iloc[row][\"path\"]}', cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    if row % 1000 == 0: print(f'row {row}')\n",
    "\n",
    "for row in range(len(val_df)):\n",
    "    df_row = val_df.iloc[row]\n",
    "    img = convertImg(val_df, 224, 'val', row)\n",
    "    cv2.imwrite(f'val2017/{val_df.iloc[row][\"path\"]}', cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    if row % 1000 == 0: print(f'row {row}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bbox columns\n",
    "del train_df['bbox']\n",
    "del val_df['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cd8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Statistics\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3745eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle files and zip data\n",
    "train_df.to_pickle('train_df.pkl')\n",
    "val_df.to_pickle('val_df.pkl')\n",
    "\n",
    "!mkdir data-2\n",
    "!mv train2017 data-2\n",
    "!mv val2017 data-2\n",
    "!(cd data-2 && zip -r -q ../data.zip .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832b75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
